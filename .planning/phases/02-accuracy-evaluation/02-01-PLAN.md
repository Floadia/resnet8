---
phase: 02-accuracy-evaluation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - scripts/evaluate.py
autonomous: true

must_haves:
  truths:
    - "Running evaluation script produces accuracy output"
    - "Per-class accuracy displayed for all 10 CIFAR-10 classes"
    - "Overall accuracy is >= 85%"
    - "Output shows total correct, total samples, percentage"
  artifacts:
    - path: "scripts/evaluate.py"
      provides: "ONNX model evaluation on CIFAR-10"
      min_lines: 50
      contains: "InferenceSession"
    - path: "requirements.txt"
      provides: "ONNX Runtime dependency"
      contains: "onnxruntime"
  key_links:
    - from: "scripts/evaluate.py"
      to: "models/resnet8.onnx"
      via: "ort.InferenceSession(model_path)"
      pattern: "InferenceSession.*onnx"
    - from: "scripts/evaluate.py"
      to: "cifar-10-batches-py/test_batch"
      via: "pickle.load"
      pattern: "pickle\\.load.*test_batch"
---

<objective>
Evaluate the ONNX ResNet8 model on CIFAR-10 test set and verify >85% accuracy

Purpose: Validate the converted model produces accurate predictions on the standard CIFAR-10 test benchmark
Output: Evaluation script that reports overall and per-class accuracy
</objective>

<execution_context>
@/home/impactaky/shelffiles/config/claude/get-shit-done/workflows/execute-plan.md
@/home/impactaky/shelffiles/config/claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-accuracy-evaluation/02-RESEARCH.md
@.planning/phases/01-model-conversion/01-01-SUMMARY.md
@scripts/convert.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add ONNX Runtime and create evaluation script</name>
  <files>requirements.txt, scripts/evaluate.py</files>
  <action>
1. Update requirements.txt to add onnxruntime>=1.23.2

2. Install the new dependency:
   venv/bin/pip install -r requirements.txt

3. Create scripts/evaluate.py following 02-RESEARCH.md patterns:
   - Load ONNX model via ort.InferenceSession
   - Get input name from session.get_inputs()[0].name
   - Load CIFAR-10 test_batch from /mnt/ext1/references/tiny/benchmark/training/image_classification/cifar-10-batches-py/
   - Use pickle.load(fo, encoding='bytes') with byte-string keys (b'data', b'labels')
   - Reshape images: (10000, 3072) -> reshape(-1, 3, 32, 32) -> transpose(0, 2, 3, 1) -> (10000, 32, 32, 3)
   - Normalize to float32 and divide by 255.0 for [0,1] range
   - Run inference on full batch (model has dynamic batch dimension)
   - Calculate overall accuracy: np.mean(predicted == labels)
   - Calculate per-class accuracy with boolean masking for each of 10 classes
   - Print results with format: class_name: correct/total = percentage
   - Load class names from batches.meta file (decode byte strings)

4. Use argparse for configurable paths:
   --model (default: models/resnet8.onnx)
   --data-dir (default: /mnt/ext1/references/tiny/benchmark/training/image_classification/cifar-10-batches-py)

IMPORTANT: Use b'data', b'labels', b'label_names' as dictionary keys (Python 3 pickle with encoding='bytes').
IMPORTANT: Convert images to float32 BEFORE dividing by 255.0 to avoid precision issues.
  </action>
  <verify>
- venv/bin/pip show onnxruntime returns version >= 1.23.2
- venv/bin/python scripts/evaluate.py --help shows usage with --model and --data-dir arguments
- venv/bin/python -c "import onnxruntime; print(onnxruntime.__version__)" succeeds
  </verify>
  <done>
- onnxruntime installed in venv
- scripts/evaluate.py exists with InferenceSession usage
- Script accepts --model and --data-dir arguments
  </done>
</task>

<task type="auto">
  <name>Task 2: Run evaluation and verify accuracy</name>
  <files>None (verification only)</files>
  <action>
Run the evaluation script on the ONNX model:

venv/bin/python scripts/evaluate.py

Expected output format:
- Model input info (name, shape)
- Number of test images (10000)
- Overall accuracy: correct/10000 = X.XX%
- Per-class accuracy for all 10 classes (airplane through truck)

If accuracy is below 85%:
- Check normalization (should be /255.0 for [0,1] range)
- Verify input shape matches model expectation
- Check argmax axis (should be axis=1)

If accuracy is around 10% (random chance):
- Normalization mismatch is likely - investigate training preprocessing
  </action>
  <verify>
- Script output shows "Overall Accuracy: X/10000 = XX.XX%"
- Per-class accuracy shown for all 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)
- Overall accuracy >= 85.0%
  </verify>
  <done>
- Evaluation completes successfully
- All 10 per-class accuracies reported
- Overall accuracy >= 85% (EVAL-03 satisfied)
  </done>
</task>

</tasks>

<verification>
Phase 2 requirements checklist:
- [ ] EVAL-01: Evaluation script runs inference on all 10,000 CIFAR-10 test images using ONNX Runtime
- [ ] EVAL-02: Per-class accuracy reported for all 10 classes
- [ ] EVAL-03: Overall accuracy >= 85% on CIFAR-10 test set

Success criteria from roadmap:
- [ ] Evaluation output includes total correct predictions, total samples, and percentage accuracy
</verification>

<success_criteria>
1. scripts/evaluate.py exists and runs without errors
2. ONNX Runtime inference completes on 10,000 test images
3. Per-class accuracy printed for: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck
4. Overall accuracy >= 85%
5. Output format: "Overall Accuracy: XXXX/10000 = XX.XX%"
</success_criteria>

<output>
After completion, create `.planning/phases/02-accuracy-evaluation/02-01-SUMMARY.md`
</output>
