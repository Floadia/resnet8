---
phase: 06-onnx-runtime-quantization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/quantize_onnx.py
autonomous: true

must_haves:
  truths:
    - "Quantized int8 ONNX model exists at models/resnet8_int8.onnx"
    - "Quantized uint8 ONNX model exists at models/resnet8_uint8.onnx"
    - "Both quantized models evaluate successfully with existing evaluate.py"
    - "Int8 accuracy delta vs 87.19% baseline is reported"
    - "Uint8 accuracy delta vs 87.19% baseline is reported"
    - "Quantization logs show MinMax calibration method and 1000 sample count"
  artifacts:
    - path: "scripts/quantize_onnx.py"
      provides: "ONNX quantization script with CalibrationDataReader"
      min_lines: 80
      contains: "quantize_static"
    - path: "models/resnet8_int8.onnx"
      provides: "Int8 quantized ONNX model"
    - path: "models/resnet8_uint8.onnx"
      provides: "Uint8 quantized ONNX model"
  key_links:
    - from: "scripts/quantize_onnx.py"
      to: "scripts/calibration_utils.py"
      via: "import load_calibration_data"
      pattern: "from calibration_utils import load_calibration_data"
    - from: "scripts/quantize_onnx.py"
      to: "onnxruntime.quantization"
      via: "quantize_static API"
      pattern: "from onnxruntime\\.quantization import.*quantize_static"
---

<objective>
Quantize ResNet8 ONNX model to int8 and uint8, then evaluate accuracy against baseline

Purpose: Produce quantized ONNX models with measured accuracy to validate PTQ effectiveness on this architecture
Output: quantize_onnx.py script, int8/uint8 ONNX models, accuracy evaluation results
</objective>

<execution_context>
@/home/impactaky/shelffiles/config/claude/get-shit-done/workflows/execute-plan.md
@/home/impactaky/shelffiles/config/claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 5 calibration infrastructure
@.planning/phases/05-calibration-infrastructure/05-01-SUMMARY.md

# Research findings
@.planning/phases/06-onnx-runtime-quantization/06-RESEARCH.md

# Existing scripts
@scripts/calibration_utils.py
@scripts/evaluate.py
@scripts/convert.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ONNX quantization script with CalibrationDataReader</name>
  <files>scripts/quantize_onnx.py</files>
  <action>
Create scripts/quantize_onnx.py that:

1. Implements CalibrationDataReader subclass wrapping calibration_utils.load_calibration_data():
   - __init__: Load calibration data (1000 samples via load_calibration_data), get model input name from ONNX metadata
   - get_next(): Return single sample dict {input_name: array of shape (1, 32, 32, 3)} or None when exhausted
   - NOTE: Need fresh reader instance for each quantization call (iterator is consumed)

2. Main quantization function that:
   - Checks if models/resnet8.onnx exists, runs scripts/convert.py if missing (prerequisite)
   - Sets np.random.seed(42) for reproducible calibration sampling
   - Logs quantization parameters (model path, activation type, calibration method, sample count)
   - Calls quantize_static() twice:
     a) Int8: QuantType.QInt8 activations and weights -> models/resnet8_int8.onnx
     b) Uint8: QuantType.QUInt8 activations and weights -> models/resnet8_uint8.onnx
   - Uses: QuantFormat.QDQ (recommended for CPU), CalibrationMethod.MinMax, per_channel=False

3. CLI arguments:
   - --model: Source ONNX model path (default: models/resnet8.onnx)
   - --output-int8: Int8 output path (default: models/resnet8_int8.onnx)
   - --output-uint8: Uint8 output path (default: models/resnet8_uint8.onnx)
   - --data-dir: CIFAR-10 data directory (default: /mnt/ext1/references/tiny/benchmark/training/image_classification/cifar-10-batches-py)
   - --samples-per-class: Calibration samples per class (default: 100)
   - --seed: Random seed (default: 42)

Key implementation notes from research:
- Import: from onnxruntime.quantization import CalibrationDataReader, quantize_static, QuantFormat, QuantType, CalibrationMethod
- Add scripts/ to sys.path to import calibration_utils
- Calibration data format: NHWC (32, 32, 3), float32, raw pixels 0-255 (matches evaluate.py)
- Create NEW CalibrationDataReader instance for each quantize_static() call (iterator exhausted after first use)

Script should print clear output showing:
- Random seed used
- Calibration samples loaded
- Each quantization step with type and output path
- Final summary with both output files
  </action>
  <verify>
python scripts/quantize_onnx.py --help shows expected arguments AND
File scripts/quantize_onnx.py exists with CalibrationDataReader class and quantize_static calls
  </verify>
  <done>
quantize_onnx.py exists with CalibrationDataReader implementation wrapping calibration_utils, quantize_static calls for both int8/uint8, and CLI interface
  </done>
</task>

<task type="auto">
  <name>Task 2: Run quantization and evaluate accuracy against baseline</name>
  <files>models/resnet8_int8.onnx, models/resnet8_uint8.onnx</files>
  <action>
Execute quantization and evaluation:

1. Ensure ONNX model exists (run convert.py if needed):
   - Check if models/resnet8.onnx exists
   - If not, run: python scripts/convert.py (uses venv if present)

2. Run quantization script:
   python scripts/quantize_onnx.py

   Expected output:
   - Logs showing seed=42, 1000 calibration samples, MinMax method
   - models/resnet8_int8.onnx created
   - models/resnet8_uint8.onnx created

3. Evaluate all three models and record results:
   a) Baseline (FP32): python scripts/evaluate.py --model models/resnet8.onnx
      - Expected: 87.19% accuracy
   b) Int8: python scripts/evaluate.py --model models/resnet8_int8.onnx
      - Record accuracy and delta vs baseline
   c) Uint8: python scripts/evaluate.py --model models/resnet8_uint8.onnx
      - Record accuracy and delta vs baseline

4. Report results in format:
   | Model | Accuracy | Delta |
   |-------|----------|-------|
   | FP32 (baseline) | 87.19% | - |
   | Int8 | XX.XX% | -Y.YY% |
   | Uint8 | XX.XX% | -Z.ZZ% |

NOTE: Accuracy drop of 0-3% is typical for well-calibrated PTQ on CNNs. Drops >5% may indicate calibration issues.
  </action>
  <verify>
ls -la models/*.onnx shows resnet8.onnx, resnet8_int8.onnx, resnet8_uint8.onnx AND
python scripts/evaluate.py --model models/resnet8_int8.onnx completes without error AND
python scripts/evaluate.py --model models/resnet8_uint8.onnx completes without error
  </verify>
  <done>
Both quantized ONNX models exist and evaluate successfully; accuracy delta reported for int8 and uint8 vs 87.19% baseline
  </done>
</task>

</tasks>

<verification>
Phase success criteria from ROADMAP.md:
1. [ ] Quantized ONNX models exist (resnet8_int8.onnx and resnet8_uint8.onnx)
2. [ ] Both quantized models evaluate successfully on CIFAR-10 test set
3. [ ] Accuracy delta reported for int8 model vs 87.19% baseline
4. [ ] Accuracy delta reported for uint8 model vs 87.19% baseline
5. [ ] Quantization script logs calibration method used (MinMax) and sample count
</verification>

<success_criteria>
- scripts/quantize_onnx.py creates int8 and uint8 ONNX models using static quantization
- models/resnet8_int8.onnx exists and loads in ONNX Runtime
- models/resnet8_uint8.onnx exists and loads in ONNX Runtime
- Both models achieve reasonable accuracy on CIFAR-10 (typically within 0-3% of baseline)
- Quantization logs show: seed=42, 1000 calibration samples, MinMax calibration method
- Accuracy comparison table with deltas from 87.19% baseline
</success_criteria>

<output>
After completion, create `.planning/phases/06-onnx-runtime-quantization/06-01-SUMMARY.md`
</output>
