---
phase: 07-pytorch-quantization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/quantize_pytorch.py
  - models/resnet8_int8.pt
  - models/resnet8_uint8.pt
autonomous: true

must_haves:
  truths:
    - "Quantized PyTorch model exists at models/resnet8_int8.pt"
    - "Quantized model evaluates successfully on CIFAR-10 test set"
    - "Accuracy delta is reported vs 87.19% baseline"
    - "uint8 model exists at models/resnet8_uint8.pt if fbgemm supports it, otherwise documented as unsupported"
  artifacts:
    - path: "scripts/quantize_pytorch.py"
      provides: "PyTorch static quantization script with model inspection and eager mode quantization"
      contains: "get_default_qconfig"
    - path: "models/resnet8_int8.pt"
      provides: "Quantized PyTorch model using fbgemm backend (int8)"
    - path: "models/resnet8_uint8.pt"
      provides: "Quantized PyTorch model using fbgemm backend (uint8) - conditional on backend support"
  key_links:
    - from: "scripts/quantize_pytorch.py"
      to: "scripts/calibration_utils.py"
      via: "load_calibration_data import"
      pattern: "from calibration_utils import load_calibration_data"
    - from: "scripts/quantize_pytorch.py"
      to: "models/resnet8.pt"
      via: "torch.load for source model"
      pattern: "torch\\.load.*resnet8\\.pt"
---

<objective>
Quantize ResNet8 PyTorch model using static quantization (eager mode) and evaluate accuracy against 87.19% baseline.

Purpose: Complete PyTorch quantization for v1.2 PTQ Evaluation milestone, enabling comparison with ONNX Runtime results (int8: 85.58%, uint8: 86.75%)
Output: Quantized PyTorch models (resnet8_int8.pt, resnet8_uint8.pt if supported) with documented accuracy and uint8 support status
</objective>

<execution_context>
@/home/impactaky/shelffiles/config/claude/get-shit-done/workflows/execute-plan.md
@/home/impactaky/shelffiles/config/claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-pytorch-quantization/07-RESEARCH.md
@.planning/phases/06-onnx-runtime-quantization/06-01-SUMMARY.md

# Key source files
@scripts/calibration_utils.py
@scripts/evaluate_pytorch.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PyTorch quantization script with model inspection mode</name>
  <files>scripts/quantize_pytorch.py</files>
  <action>
Create scripts/quantize_pytorch.py that implements PyTorch eager mode static quantization:

**Script structure:**
1. `--inspect-only` mode to print model structure (for identifying fusion patterns)
2. `load_pytorch_model()` - loads model from checkpoint, sets eval mode
3. `create_calibration_loader()` - wraps calibration_utils.load_calibration_data(), creates DataLoader
4. `quantize_model_eager()` - implements fusion → prepare → calibrate → convert workflow

**Key implementation details:**
- Use `torch.ao.quantization` APIs: `get_default_qconfig('fbgemm')`, `prepare()`, `convert()`
- fbgemm backend uses quint8 activations + qint8 weights by default (this IS int8 quantization)
- Model expects NHWC input format (verified in evaluate_pytorch.py) - DO NOT transpose calibration data
- Start WITHOUT layer fusion to establish baseline (onnx2torch model structure unknown)
- Use batch_size=32 for calibration (matches research recommendation)
- Set random seed to 42 for reproducibility (matches calibration_utils)

**Model inspection output:**
```python
for name, module in model.named_modules():
    print(f"{name:40s} {type(module).__name__}")
```

**Quantization workflow (no fusion initially):**
```python
model.qconfig = get_default_qconfig('fbgemm')
prepared_model = prepare(model, inplace=False)
# Calibration loop with 1000 samples
for data, _ in calibration_loader:
    prepared_model(data)
quantized_model = convert(prepared_model, inplace=False)
```

**Save format:** `torch.save({'model': quantized_model}, output_path)` to match existing evaluate_pytorch.py loader

**Arguments:**
- `--model` (default: models/resnet8.pt)
- `--output` (default: models/resnet8_int8.pt)
- `--data-dir` (default: /mnt/ext1/references/tiny/benchmark/training/image_classification/cifar-10-batches-py)
- `--samples-per-class` (default: 100)
- `--batch-size` (default: 32)
- `--inspect-only` (flag: only print model structure, no quantization)

**What NOT to do:**
- Do NOT add QuantStub/DeQuantStub wrappers initially (try without first)
- Do NOT define fusion patterns initially (model structure unknown, may not need fusion)
- Do NOT transpose calibration data to NCHW (model uses NHWC)
- Do NOT use PT2E/torch.export (use simpler eager mode APIs)
  </action>
  <verify>
1. `python scripts/quantize_pytorch.py --help` shows all expected arguments
2. `python scripts/quantize_pytorch.py --inspect-only` prints model structure without errors
  </verify>
  <done>
quantize_pytorch.py exists with --inspect-only mode working, prints onnx2torch model structure
  </done>
</task>

<task type="auto">
  <name>Task 2: Run quantization and evaluate accuracy against baseline</name>
  <files>models/resnet8_int8.pt, models/resnet8_uint8.pt</files>
  <action>
Run the quantization workflow and evaluate results:

**Step 1: Attempt int8 quantization**
```bash
python scripts/quantize_pytorch.py --model models/resnet8.pt --output models/resnet8_int8.pt
```

**If quantization fails (common issues from research):**
1. **Missing observers error:** May need QuantStub/DeQuantStub wrappers. Update script to wrap model in quantization-compatible class.
2. **FloatFunctional needed:** If skip connections exist, replace `+` with `FloatFunctional.add()`. Update script to detect and wrap skip connections.
3. **Fusion errors:** If fuse_modules() was attempted and failed, remove fusion (already planned to skip initially).

Document any modifications required to make quantization work.

**Step 2: Evaluate int8 quantized model**
```bash
python scripts/evaluate_pytorch.py --model models/resnet8_int8.pt
```

**Step 3: Attempt uint8 quantization (if fbgemm supports it)**
fbgemm backend default is quint8 activations + qint8 weights. To create a pure uint8 model:
- Check if fbgemm supports uint8-only quantization via custom qconfig with `quint8` for both weights and activations
- If supported: Create custom qconfig and run quantization:
  ```bash
  python scripts/quantize_pytorch.py --model models/resnet8.pt --output models/resnet8_uint8.pt --dtype uint8
  ```
  (Add --dtype argument to script if uint8 is supported)
- If NOT supported: Document in script output: "fbgemm backend does not support uint8-only weight quantization (weights require qint8)"

**Step 4: Evaluate uint8 model (if created)**
```bash
python scripts/evaluate_pytorch.py --model models/resnet8_uint8.pt
```

**Step 5: Compare against baselines**
| Model | Expected | Source |
|-------|----------|--------|
| FP32 baseline | 87.19% | Phase 4 evaluation |
| ONNX Runtime int8 | 85.58% | Phase 6 |
| ONNX Runtime uint8 | 86.75% | Phase 6 |

Calculate deltas:
- PyTorch int8 accuracy - 87.19%
- PyTorch uint8 accuracy - 87.19% (if created)

**Success threshold:** Quantized model accuracy > 80% (allowing for larger drop than ONNX Runtime if onnx2torch model structure causes issues)
  </action>
  <verify>
1. `ls -la models/resnet8_int8.pt` shows file exists
2. `python scripts/evaluate_pytorch.py --model models/resnet8_int8.pt` runs and reports accuracy
3. Accuracy reported vs 87.19% baseline (delta calculated)
4. uint8 support check completed:
   - If supported: `ls -la models/resnet8_uint8.pt` shows file exists AND `python scripts/evaluate_pytorch.py --model models/resnet8_uint8.pt` reports accuracy
   - If not supported: Script output documents "fbgemm does not support uint8-only quantization"
  </verify>
  <done>
resnet8_int8.pt exists, evaluates successfully with accuracy delta vs 87.19% reported. uint8 either: (a) resnet8_uint8.pt exists with accuracy evaluated, OR (b) documented as unsupported by fbgemm backend
  </done>
</task>

</tasks>

<verification>
Phase 7 success criteria from ROADMAP.md:
1. [x] Quantized PyTorch model exists (resnet8_int8.pt) - verified by `ls models/resnet8_int8.pt`
2. [x] uint8 model exists if fbgemm backend supports it, otherwise documented as unsupported
3. [x] Quantized models evaluate successfully on CIFAR-10 test set
4. [x] Accuracy delta reported for int8 model vs 87.19% baseline
5. [x] Accuracy delta reported for uint8 model (if created) vs 87.19% baseline

Requirements coverage:
- PTQ-01: PyTorch model quantized to int8 using static quantization
- PTQ-02: PyTorch model quantized to uint8 using static quantization (if supported)
- PTQ-03: Quantized PyTorch models evaluated on CIFAR-10 test set
- PTQ-04: Accuracy delta reported vs 87.19% baseline
</verification>

<success_criteria>
- scripts/quantize_pytorch.py exists with --inspect-only mode
- models/resnet8_int8.pt exists and loads without errors
- Quantized model achieves >80% accuracy on CIFAR-10 test set
- Accuracy delta vs 87.19% baseline is documented
- uint8 support status documented: either models/resnet8_uint8.pt exists with accuracy evaluated, OR documented as unsupported by fbgemm
</success_criteria>

<output>
After completion, create `.planning/phases/07-pytorch-quantization/07-01-SUMMARY.md`
</output>
