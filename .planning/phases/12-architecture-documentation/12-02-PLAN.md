---
phase: 12-architecture-documentation
plan: 02
type: execute
wave: 2
depends_on: ["12-01"]
files_modified:
  - docs/quantization/04-architecture.md
autonomous: true

must_haves:
  truths:
    - "Residual connection handling documented with scale mismatch problem explained (different branches may have different scales)"
    - "Solution approaches compared (QDQ dequant-add-quant vs scale matching vs PyTorch FloatFunctional)"
    - "PyTorch quantized operation equivalents mapped to ONNX QDQ patterns"
    - "Known PyTorch→ONNX conversion limitations documented with recommended workflow"
  artifacts:
    - path: "docs/quantization/04-architecture.md"
      provides: "Residual connections and PyTorch equivalents sections"
      contains: ["Residual", "Add", "scale mismatch", "torch.nn.quantized", "FloatFunctional"]
  key_links:
    - from: "docs/quantization/04-architecture.md"
      to: "ResNet8 Add operations"
      via: "ONNX graph inspection"
      pattern: "Add"
    - from: "docs/quantization/04-architecture.md"
      to: "docs/quantization/02-qlinearconv.md"
      via: "cross-reference for QLinearConv math"
      pattern: "02-qlinearconv"
---

<objective>
Document residual connection handling in quantized ResNet8 and map PyTorch quantized operations to ONNX QDQ patterns.

Purpose: Complete the architecture documentation by addressing the critical residual connection scale mismatch problem and enabling users to understand PyTorch↔ONNX quantization equivalences.

Output:
- Residual connections section added to architecture documentation
- PyTorch equivalents section with mapping table and conversion notes
</objective>

<execution_context>
@/home/impactaky/shelffiles/config/claude/get-shit-done/workflows/execute-plan.md
@/home/impactaky/shelffiles/config/claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-architecture-documentation/12-RESEARCH.md

# Prior plan output (required - Plan 01 creates base documentation)
@.planning/phases/12-architecture-documentation/12-01-SUMMARY.md

# Core operations documentation to cross-reference
@docs/quantization/02-qlinearconv.md
@docs/quantization/03-qlinearmatmul.md

# Architecture documentation created in Plan 01
@docs/quantization/04-architecture.md

# Extraction script for inspecting Add nodes
@scripts/extract_operations.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Document residual connection handling</name>
  <files>docs/quantization/04-architecture.md</files>
  <action>
Add a new section "## Residual Connections in Quantized Networks" to the architecture documentation with:

### 5. Residual Connection Handling

**5.1 The Scale Mismatch Problem**
- In ResNet, residual connections add two branches: `output = conv_branch + skip_branch`
- In quantized networks, each branch may have different scales
- Direct INT8 addition with different scales produces incorrect results
- Example: If scale_conv=0.1 and scale_skip=0.05, adding INT8 values directly is mathematically wrong

**5.2 QDQ Solution (Used in ResNet8)**
Inspect the quantized ResNet8 ONNX model to identify Add operations:
1. Use extract_operations.py or direct ONNX inspection to find all Add nodes
2. Trace inputs to each Add node to find feeding DequantizeLinear nodes
3. Extract scale values from both branches

Pattern in QDQ format:
```
Branch 1: ... → DequantizeLinear(scale1) → \
                                            Add(FP32) → QuantizeLinear(output_scale) → ...
Branch 2: ... → DequantizeLinear(scale2) → /
```

**How it works:**
- Dequantize both branches to FP32 using their respective scales
- Perform addition in floating-point (mathematically correct)
- Quantize result back to INT8 with output scale

**Trade-off:** Requires FP32 arithmetic for Add operation

**5.3 Alternative Approaches (For Reference)**

Document other approaches for completeness:

1. **Scale Matching** - Force both branches to use identical scales during calibration
   - Pro: Pure INT8 addition possible
   - Con: Constrains calibration, may reduce accuracy

2. **INT8 Addition with Rescaling** - Rescale one branch before adding
   - Pro: No FP32 required
   - Con: Additional rounding error, complex implementation

**5.4 ResNet8 Specific Analysis**
- List all Add operations in ResNet8 with their input branch scales
- Show which Q/DQ pairs surround each Add
- Note: ResNet8 has 3 residual stacks, each with Add operations

Use extract_operations.py to gather actual scale values from model.
  </action>
  <verify>
Check section exists: `grep -c "Residual Connection" docs/quantization/04-architecture.md` (should be >= 1)
Check scale mismatch documented: `grep -c "scale mismatch\|different scale" docs/quantization/04-architecture.md` (should be >= 1)
Check solution documented: `grep -c "DequantizeLinear.*Add\|Add.*FP32" docs/quantization/04-architecture.md` (should be >= 1)
  </verify>
  <done>
Residual connection handling documented with scale mismatch problem explained, QDQ solution shown with pattern, and alternative approaches listed for reference.
  </done>
</task>

<task type="auto">
  <name>Task 2: Document PyTorch quantized operation equivalents</name>
  <files>docs/quantization/04-architecture.md</files>
  <action>
Add a new section "## PyTorch Quantized Operation Equivalents" to the architecture documentation:

### 6. PyTorch Quantized Operation Equivalents

**6.1 Mapping Table**

Create mapping table showing PyTorch → ONNX QDQ equivalents:

| PyTorch Quantized Op | ONNX QDQ Pattern |
|---------------------|------------------|
| torch.nn.quantized.Conv2d | QuantizeLinear → DequantizeLinear → Conv → QuantizeLinear → DequantizeLinear |
| torch.nn.quantized.Linear | QuantizeLinear → DequantizeLinear → MatMul → QuantizeLinear → DequantizeLinear |
| torch.nn.quantized.ReLU | QuantizeLinear → DequantizeLinear → Relu → QuantizeLinear → DequantizeLinear |
| torch.ao.nn.quantized.FloatFunctional.add | DequantizeLinear (×2) → Add → QuantizeLinear |

**6.2 Important Notes**

Document key points from research:
1. **Conceptual mapping, not direct export**: These operations are conceptually equivalent but direct PyTorch quantized model export to ONNX has limitations
2. **Recommended workflow**: Export FP32 PyTorch model → Quantize with ONNX Runtime tools
3. **Why export is limited**: `aten::quantize_per_channel` not supported in ONNX opset versions

**6.3 FloatFunctional for Residual Connections**

Explain PyTorch's approach:
```python
# In model definition:
self.add = torch.ao.nn.quantized.FloatFunctional()

# In forward pass:
out = self.add.add(branch1, branch2)
```

- Wraps add operation to track quantization statistics
- Automatically handles scale/zero-point mismatches
- Exports to ONNX as QDQ pattern (same as Section 5.2)

**6.4 Cross-Reference to Core Operations**

Add links to QLinearConv and QLinearMatMul documentation:
- For detailed INT8×INT8→INT32 computation, see [02-qlinearconv.md](02-qlinearconv.md)
- For matrix multiplication details, see [03-qlinearmatmul.md](03-qlinearmatmul.md)

Note: QLinear operators (QLinearConv, QLinearMatMul) are ONNX specification operations that represent the computation INSIDE the QDQ pattern. The two-stage computation documented in Phase 11 applies whether using QLinear operators directly or QDQ format.
  </action>
  <verify>
Check section exists: `grep -c "PyTorch.*Equivalent\|torch.nn.quantized" docs/quantization/04-architecture.md` (should be >= 2)
Check mapping table: `grep -c "Conv2d\|Linear\|FloatFunctional" docs/quantization/04-architecture.md` (should be >= 3)
Check limitations documented: `grep -c "limitation\|not supported\|recommended workflow" docs/quantization/04-architecture.md` (should be >= 1)
Check cross-references: `grep -c "02-qlinearconv\|03-qlinearmatmul" docs/quantization/04-architecture.md` (should be >= 1)
  </verify>
  <done>
PyTorch equivalents documented with mapping table, conversion limitations noted, FloatFunctional explained, and cross-references to core operations added.
  </done>
</task>

</tasks>

<verification>
1. Architecture documentation includes "Residual Connection" section
2. Scale mismatch problem explained with example
3. QDQ solution pattern documented (DequantizeLinear → Add → QuantizeLinear)
4. PyTorch mapping table present with at least 4 operations
5. Conversion limitations documented
6. Cross-references to QLinearConv and QLinearMatMul documentation
7. All 4 requirements (ARCH-01 through ARCH-04) covered between Plans 01 and 02
</verification>

<success_criteria>
- Residual connection scale mismatch problem clearly explained
- QDQ solution documented showing how Add operations handle mismatched scales
- Alternative approaches (scale matching) mentioned for completeness
- PyTorch → ONNX QDQ mapping table covers Conv2d, Linear, ReLU, FloatFunctional
- Conversion limitations documented with recommended workflow (export FP32 → quantize with ONNX Runtime)
- Cross-references to core operations documentation (avoid duplication)
</success_criteria>

<output>
After completion, create `.planning/phases/12-architecture-documentation/12-02-SUMMARY.md`
</output>
