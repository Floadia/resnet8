---
phase: 05-calibration-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/calibration_utils.py
autonomous: true

must_haves:
  truths:
    - "Calibration dataset contains 1000 stratified samples (100 per class)"
    - "Calibration preprocessing matches evaluation exactly (raw 0-255, no normalization)"
    - "Class distribution is balanced (each of 10 classes has exactly 100 samples)"
  artifacts:
    - path: "scripts/calibration_utils.py"
      provides: "Stratified calibration data loader"
      exports: ["load_calibration_data", "verify_distribution"]
      min_lines: 60
  key_links:
    - from: "scripts/calibration_utils.py"
      to: "cifar-10-batches-py/data_batch_*"
      via: "pickle.load from training batches"
      pattern: "data_batch_"
    - from: "scripts/calibration_utils.py"
      to: "scripts/evaluate.py"
      via: "identical preprocessing (raw pixels 0-255, NHWC)"
      pattern: "astype.*float32"
---

<objective>
Create calibration data infrastructure for Post-Training Quantization.

Purpose: Both ONNX Runtime and PyTorch static quantization require representative calibration data to compute optimal quantization parameters. Incorrect calibration causes 20-70% accuracy drops.

Output: `scripts/calibration_utils.py` providing stratified CIFAR-10 calibration samples with preprocessing identical to evaluation pipeline.
</objective>

<execution_context>
@/home/impactaky/shelffiles/config/claude/get-shit-done/workflows/execute-plan.md
@/home/impactaky/shelffiles/config/claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Critical reference for preprocessing pattern:
@scripts/evaluate.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create calibration data loader with stratified sampling</name>
  <files>scripts/calibration_utils.py</files>
  <action>
Create `scripts/calibration_utils.py` with two main functions:

1. `load_calibration_data(data_dir: str, samples_per_class: int = 100) -> tuple[np.ndarray, np.ndarray, list[str]]`
   - Load CIFAR-10 TRAINING batches (data_batch_1 through data_batch_5), NOT test_batch
   - Stratified sampling: exactly `samples_per_class` samples from each of the 10 classes
   - Default 100 samples/class = 1000 total samples (exceeds roadmap minimum of 200)
   - Preprocessing MUST match evaluate.py exactly:
     - Reshape from (N, 3072) to (N, 32, 32, 3) via reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)
     - Convert to float32 WITHOUT normalization (raw pixel values 0-255)
     - Do NOT divide by 255 (this was the v1.0 preprocessing bug)
   - Return (images, labels, class_names) tuple

2. `verify_distribution(labels: np.ndarray, class_names: list[str]) -> dict[str, int]`
   - Count samples per class
   - Return dict mapping class_name to count
   - Print distribution summary

Add CLI entry point with argparse:
- `--data-dir` argument (default: `/mnt/ext1/references/tiny/benchmark/training/image_classification/cifar-10-batches-py`)
- `--samples-per-class` argument (default: 100)
- When run directly, loads data and prints verification

Include docstrings and type hints following existing scripts convention.
  </action>
  <verify>
Run: `python scripts/calibration_utils.py --samples-per-class 100`
Expected output:
- "Loaded 1000 calibration samples"
- Distribution table showing 100 samples per class (all 10 classes)
- No normalization warnings
  </verify>
  <done>
- calibration_utils.py exists with load_calibration_data and verify_distribution functions
- Running script outputs exactly 1000 samples with 100 per class
- Preprocessing matches evaluate.py (float32, 0-255 range, NHWC format)
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify calibration data matches evaluation preprocessing</name>
  <files>scripts/calibration_utils.py</files>
  <action>
Verify the calibration data preprocessing matches evaluation exactly:

1. Run calibration_utils.py and capture output
2. Check that:
   - Image dtype is float32
   - Image shape is (N, 32, 32, 3) - NHWC format
   - Value range is 0-255 (NOT 0-1)
   - Distribution is balanced (100 per class)

3. Add a verification check to calibration_utils.py that prints:
   - Image dtype
   - Image shape
   - Min/max pixel values (should be ~0 to ~255)
   - Shape format confirmation ("NHWC format confirmed")

This ensures future quantization phases will use correctly preprocessed data.
  </action>
  <verify>
Run: `python scripts/calibration_utils.py`
Output must include:
- "dtype: float32"
- "shape: (1000, 32, 32, 3)"
- "pixel range: [0.0, 255.0]" (or close to it)
- "NHWC format confirmed"
- 100 samples per class
  </verify>
  <done>
- Calibration data verified to match evaluation preprocessing
- Dtype is float32, shape is NHWC, values are 0-255
- Distribution is balanced across all 10 classes
  </done>
</task>

</tasks>

<verification>
Phase success verified when:

1. **CAL-01 (Calibration dataset):**
   ```bash
   python scripts/calibration_utils.py --samples-per-class 100
   # Outputs: "Loaded 1000 calibration samples"
   # Distribution: 100 per class
   ```

2. **CAL-02 (Preprocessing matches):**
   ```bash
   python scripts/calibration_utils.py
   # Outputs: dtype=float32, shape=(N,32,32,3), range=[0,255]
   ```

3. **Sanity check** - compare with evaluation preprocessing:
   ```bash
   grep -n "astype.*float32" scripts/evaluate.py scripts/calibration_utils.py
   # Both files should have identical preprocessing pattern
   ```
</verification>

<success_criteria>
- [ ] scripts/calibration_utils.py exists and is executable
- [ ] load_calibration_data returns 1000 samples (100 per class)
- [ ] Images are float32, shape (N, 32, 32, 3), values 0-255
- [ ] verify_distribution confirms balanced class distribution
- [ ] No normalization applied (matches evaluate.py preprocessing)
</success_criteria>

<output>
After completion, create `.planning/phases/05-calibration-infrastructure/05-01-SUMMARY.md`
</output>
