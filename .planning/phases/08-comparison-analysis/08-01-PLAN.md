---
phase: 08-comparison-analysis
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [docs/QUANTIZATION_ANALYSIS.md]
autonomous: true

must_haves:
  truths:
    - "Reader can compare accuracy across all quantization configurations at a glance"
    - "Configurations with >5% accuracy drop are clearly flagged"
    - "Model size reduction from quantization is quantified"
    - "Best quantization approach is recommended with justification"
  artifacts:
    - path: "docs/QUANTIZATION_ANALYSIS.md"
      provides: "Complete PTQ evaluation analysis document"
      contains: "Framework.*Data Type.*Accuracy.*Delta"
  key_links:
    - from: "docs/QUANTIZATION_ANALYSIS.md"
      to: "models/*.onnx, models/*.pt"
      via: "file size references"
      pattern: "(resnet8_int8|resnet8_uint8)"
---

<objective>
Create a comprehensive quantization analysis document comparing all PTQ results from ONNX Runtime and PyTorch frameworks.

Purpose: Document the v1.2 PTQ Evaluation milestone findings and provide clear recommendation for deployment
Output: docs/QUANTIZATION_ANALYSIS.md with comparison table, accuracy analysis, size comparison, and recommendation
</objective>

<execution_context>
@/home/impactaky/shelffiles/config/claude/get-shit-done/workflows/execute-plan.md
@/home/impactaky/shelffiles/config/claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-onnx-runtime-quantization/06-01-SUMMARY.md
@.planning/phases/07-pytorch-quantization/07-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create quantization comparison analysis document</name>
  <files>docs/QUANTIZATION_ANALYSIS.md</files>
  <action>
Create docs/QUANTIZATION_ANALYSIS.md with the following structure:

**1. Executive Summary**
- Brief overview of PTQ evaluation scope (ONNX Runtime and PyTorch)
- Key finding: Best accuracy retention achieved by [identify from data]
- Deployment recommendation: [derive from analysis]

**2. Comparison Table (ANL-01)**
Create a markdown table with columns:
| Framework | Data Type | Accuracy | Delta vs Baseline | Model Size | Size Reduction |

Include these rows:
- FP32 Baseline (ONNX): 87.19%, 315KB, reference
- FP32 Baseline (PyTorch): 87.19%, 353KB, reference
- ONNX Runtime int8: 85.58%, 123KB, -1.61% accuracy, 61% size reduction
- ONNX Runtime uint8: 86.75%, 123KB, -0.44% accuracy, 61% size reduction
- PyTorch int8: 85.68%, 168KB, -1.51% accuracy, 52% size reduction
- PyTorch uint8: N/A (document as "Not supported - fbgemm requires qint8 weights")

**3. Accuracy Analysis (ANL-02)**
- List any configurations with >5% accuracy drop (should be none based on data)
- Explicitly state: "No configurations exceed the 5% accuracy drop threshold"
- Note per-class accuracy findings if relevant (dog class drop in int8)

**4. Size Comparison**
- Original model sizes: ONNX 315KB, PyTorch 353KB
- Quantized sizes and reduction percentages
- Note: PyTorch int8 larger than ONNX due to TorchScript format overhead

**5. Framework Comparison**
- ONNX Runtime: Supports both int8 and uint8, better uint8 accuracy
- PyTorch: Only int8 supported (fbgemm limitation), comparable to ONNX int8
- Calibration: Both used same 1000-sample stratified dataset

**6. Recommendation**
Based on analysis:
- For best accuracy: ONNX Runtime uint8 (86.75%, -0.44% drop)
- For PyTorch deployment: int8 acceptable (85.68%, -1.51% drop)
- All quantized models meet >85% accuracy threshold
- All quantized models meet <5% accuracy drop requirement

**7. Methodology Notes**
- Calibration: 1000 stratified samples (100 per class) from CIFAR-10 training set
- Quantization type: Static (post-training)
- Baseline: 87.19% on CIFAR-10 test set (10,000 images)
  </action>
  <verify>
- File exists: `ls docs/QUANTIZATION_ANALYSIS.md`
- Contains comparison table: `grep -c "Framework.*Data Type" docs/QUANTIZATION_ANALYSIS.md` returns 1
- Contains all model entries: `grep -c "int8\|uint8" docs/QUANTIZATION_ANALYSIS.md` returns at least 5
- Contains accuracy drop flag section: `grep -c "5%" docs/QUANTIZATION_ANALYSIS.md` returns at least 1
- Contains recommendation: `grep -c "Recommendation" docs/QUANTIZATION_ANALYSIS.md` returns at least 1
  </verify>
  <done>
Comparison analysis document exists with:
- Complete comparison table (Framework x Data Type x Accuracy x Delta x Size)
- Explicit statement about no >5% accuracy drops
- Model size comparison with reduction percentages
- Clear recommendation for best quantization approach
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify all Phase 8 success criteria are met</name>
  <files>docs/QUANTIZATION_ANALYSIS.md</files>
  <action>
Validate the analysis document against all Phase 8 success criteria from ROADMAP.md:

1. **Criterion 1:** "Comparison table exists showing: Framework x Data Type x Accuracy x Delta from baseline"
   - Verify table contains all 5 columns
   - Verify all configurations are listed (4 quantized + 2 baseline)

2. **Criterion 2:** "All configurations with accuracy drop >5% are flagged in analysis"
   - Verify explicit statement about >5% check exists
   - Verify no false negatives (all data shows <5% drop)

3. **Criterion 3:** "Model size comparison included (FP32 vs quantized for all models)"
   - Verify baseline sizes (315KB ONNX, 353KB PyTorch) are listed
   - Verify quantized sizes (123KB ONNX, 168KB PyTorch) are listed
   - Verify reduction percentages are calculated

4. **Criterion 4:** "Analysis document includes recommendation for best quantization approach"
   - Verify Recommendation section exists
   - Verify specific recommendation is given (ONNX Runtime uint8 for best accuracy)

If any criteria not met, update the document to address gaps.
  </action>
  <verify>
Run checklist verification:
```bash
echo "=== Phase 8 Success Criteria Verification ===" && \
echo "1. Comparison table with all columns:" && \
grep -E "Framework.*Data Type.*Accuracy.*Delta" docs/QUANTIZATION_ANALYSIS.md && \
echo "2. >5% accuracy drop check:" && \
grep -i "5%.*drop\|exceed.*5%" docs/QUANTIZATION_ANALYSIS.md && \
echo "3. Model sizes (FP32):" && \
grep -E "315.*KB|353.*KB" docs/QUANTIZATION_ANALYSIS.md && \
echo "4. Recommendation section:" && \
grep -A2 "## Recommendation" docs/QUANTIZATION_ANALYSIS.md && \
echo "=== All criteria verified ==="
```
  </verify>
  <done>
All four Phase 8 success criteria from ROADMAP.md are verified:
1. Comparison table complete with Framework x Data Type x Accuracy x Delta
2. >5% accuracy drop check explicitly documented
3. FP32 vs quantized size comparison included
4. Best quantization approach recommended
  </done>
</task>

</tasks>

<verification>
Phase 8 verification checklist:
- [ ] docs/QUANTIZATION_ANALYSIS.md exists
- [ ] Comparison table has all required columns
- [ ] All 6 configurations documented (2 baselines + 4 quantized)
- [ ] No configurations flagged for >5% drop (none expected)
- [ ] Model sizes documented with reduction percentages
- [ ] Recommendation section with clear guidance
- [ ] ANL-01 requirement satisfied (comparison table)
- [ ] ANL-02 requirement satisfied (>5% drop flagging)
</verification>

<success_criteria>
Phase 8 complete when:
1. Analysis document exists at docs/QUANTIZATION_ANALYSIS.md
2. All four ROADMAP success criteria verified
3. ANL-01 and ANL-02 requirements covered
4. v1.2 PTQ Evaluation milestone can be marked complete
</success_criteria>

<output>
After completion, create `.planning/phases/08-comparison-analysis/08-01-SUMMARY.md`
</output>
