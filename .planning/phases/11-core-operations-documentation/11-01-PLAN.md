---
phase: 11-core-operations-documentation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docs/quantization/02-qlinearconv.md
  - scripts/validate_qlinearconv.py
  - models/resnet8_int8.onnx
  - models/resnet8_int8_operations.json
autonomous: true

must_haves:
  truths:
    - "QLinearConv formula explicitly shows two-stage computation (INT8×INT8→INT32 MAC, then requantization)"
    - "All 9 inputs documented with types, shapes, and purposes"
    - "Per-tensor and per-channel quantization cases explained with actual ResNet8 values"
    - "Documentation explicitly states which ResNet8 layers use per-channel vs per-tensor"
    - "INT32 accumulator requirement proven via overflow demonstration code"
    - "Validation script confirms manual calculations match ONNX Runtime output"
  artifacts:
    - path: "docs/quantization/02-qlinearconv.md"
      provides: "Complete QLinearConv reference documentation"
      contains: "two-stage computation"
    - path: "scripts/validate_qlinearconv.py"
      provides: "Verification script for QLinearConv calculations"
      contains: "np.allclose"
    - path: "models/resnet8_int8.onnx"
      provides: "Quantized ONNX model for extracting actual values"
    - path: "models/resnet8_int8_operations.json"
      provides: "Extracted quantization parameters including scale shapes"
  key_links:
    - from: "docs/quantization/02-qlinearconv.md"
      to: "ONNX QLinearConv specification"
      via: "exact formula notation"
      pattern: "y_scale.*y_zero_point"
    - from: "scripts/validate_qlinearconv.py"
      to: "resnet8_int8.onnx"
      via: "onnx.load and ONNX Runtime"
      pattern: "onnx\\.load|InferenceSession"
    - from: "docs/quantization/02-qlinearconv.md"
      to: "models/resnet8_int8_operations.json"
      via: "per-channel vs per-tensor analysis"
      pattern: "per-channel|per-tensor"
---

<objective>
Create comprehensive QLinearConv documentation for analog accelerator implementers, covering the two-stage computation pattern with actual ResNet8 values.

Purpose: QLinearConv is the primary operation in quantized ResNet8 (all 8 convolution layers use it). Hardware implementers need to understand exact calculations.
Output: Complete documentation file with formulas, worked examples, and validation code.
</objective>

<execution_context>
@/home/impactaky/shelffiles/config/claude/get-shit-done/workflows/execute-plan.md
@/home/impactaky/shelffiles/config/claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-core-operations-documentation/11-CONTEXT.md
@.planning/phases/11-core-operations-documentation/11-RESEARCH.md
@.planning/phases/10-boundary-operations-documentation/10-01-SUMMARY.md

# Reference existing documentation pattern
@docs/quantization/01-boundary-operations.md

# Phase 9 scripts for extracting ONNX values
@scripts/extract_operations.py
@scripts/quantize_onnx.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Generate ONNX model and extract quantization parameters</name>
  <files>models/resnet8_int8.onnx, models/resnet8_int8_operations.json</files>
  <action>
**CRITICAL: User decision is LOCKED - "Use actual INT8 values from the exported ONNX model"**

Generate the quantized ONNX model if it doesn't exist, then extract operations to determine per-channel vs per-tensor quantization per layer:

1. **Check if ONNX model exists:**
   ```bash
   test -f models/resnet8_int8.onnx && echo "EXISTS" || echo "MISSING"
   ```

2. **If MISSING, generate the ONNX model:**
   ```bash
   # Requires: models/resnet8.pt (float model), data/cifar-10-batches-py (calibration data)
   python scripts/quantize_onnx.py \
     --model models/resnet8.onnx \
     --output-int8 models/resnet8_int8.onnx \
     --data-dir data/cifar-10-batches-py
   ```

   Note: If `models/resnet8.onnx` doesn't exist, first export from PyTorch:
   ```bash
   python -c "
   import torch
   from model import ResNet8  # or wherever the model is defined
   model = ResNet8()
   model.load_state_dict(torch.load('models/resnet8.pt'))
   model.eval()
   dummy_input = torch.randn(1, 3, 32, 32)
   torch.onnx.export(model, dummy_input, 'models/resnet8.onnx', opset_version=13)
   "
   ```

3. **Extract quantization parameters:**
   ```bash
   python scripts/extract_operations.py \
     --model models/resnet8_int8.onnx \
     --output models/resnet8_int8_operations.json
   ```

4. **Analyze per-channel vs per-tensor for each QLinearConv:**
   - Read `models/resnet8_int8_operations.json`
   - For each QLinearConv node, check the shape of w_scale:
     - If w_scale is a scalar (single value): **per-tensor quantization**
     - If w_scale is an array [M] where M = output channels: **per-channel quantization**
   - Create a summary table for documentation:
     ```
     | Layer | Output Channels | Quantization Type | w_scale shape |
     |-------|-----------------|-------------------|---------------|
     | conv1 | 16              | per-tensor        | scalar        |
     | conv2 | 16              | per-channel       | [16]          |
     ...
     ```
  </action>
  <verify>
1. ONNX model exists: `test -f models/resnet8_int8.onnx`
2. Operations JSON exists: `test -f models/resnet8_int8_operations.json`
3. JSON contains QLinearConv entries: `grep -q "QLinearConv" models/resnet8_int8_operations.json`
4. JSON contains scale information: `grep -q "scales" models/resnet8_int8_operations.json`
  </verify>
  <done>
- ONNX model exists at models/resnet8_int8.onnx
- Operations JSON exists at models/resnet8_int8_operations.json
- Per-channel vs per-tensor classification completed for all QLinearConv layers
- Summary table ready for inclusion in documentation
  </done>
</task>

<task type="auto">
  <name>Task 2: Create QLinearConv documentation with two-stage computation</name>
  <files>docs/quantization/02-qlinearconv.md</files>
  <action>
Create comprehensive QLinearConv documentation following the established pattern from 01-boundary-operations.md:

**Structure:**
1. **Overview** (3-4 sentences)
   - What QLinearConv does (quantized convolution)
   - Relationship to standard Conv: "same spatial computation, different arithmetic"
   - Key difference: two-stage computation with INT32 accumulator

2. **Input Specification** (table format)
   - All 9 inputs with exact ONNX names: x, x_scale, x_zero_point, w, w_scale, w_zero_point, y_scale, y_zero_point, B (optional bias)
   - Types, shapes, descriptions for each
   - Note: bias (B) is optional (8 required, 1 optional)

3. **Two-Stage Computation Formula**
   - Use GitHub MathJax syntax (escape underscores: y\_scale)
   - Stage 1: INT8×INT8→INT32 MAC operations
     $$\text{acc}_{INT32} = \sum_{k} (x_{INT8}[k] - x\_zero\_point) \times (w_{INT8}[k] - w\_zero\_point)$$
   - Stage 2: Requantization to INT8
     $$y_{INT8} = \text{saturate}\left(\text{round}\left(\text{acc} \times \frac{x\_scale \times w\_scale}{y\_scale}\right) + y\_zero\_point\right)$$
   - Explain WHY this pattern: separates integer ops (hardware-efficient) from scaling (precision-critical)

4. **Per-Channel vs Per-Tensor in ResNet8** (CORE-01 requirement)
   - **MANDATORY:** Include the per-channel vs per-tensor table from Task 1
   - Document which specific layers use which approach
   - Explain: Per-tensor has single scale, per-channel has scale per output channel
   - Show the actual w_scale values from `models/resnet8_int8_operations.json`

5. **Per-Tensor Quantization Example**
   - Use a per-tensor QLinearConv layer from ResNet8 (identified in Task 1)
   - Show actual values from ONNX model
   - Step through:
     - Input patch extraction (3×3×C values)
     - Zero-point subtraction for input and weights
     - INT32 MAC accumulation (show partial sums)
     - Scale factor computation
     - Requantization with saturation
   - Include Python code that readers can run

6. **Per-Channel Quantization Example**
   - Use a per-channel QLinearConv layer from ResNet8 (identified in Task 1)
   - Show w_scale as array [M] not scalar
   - Demonstrate: Stage 1 (MAC) is identical, only Stage 2 differs per channel
   - Storage overhead calculation: 2*M / (M*C*kH*kW) ≈ negligible

7. **INT32 Accumulator Overflow Demonstration**
   - Include runnable code from RESEARCH.md Pattern 3 (demonstrate_accumulator_overflow)
   - Show worst case: 127 × 127 × (channels × kernel_size) = overflow for INT16
   - For ResNet8: 3×3×64 = 576 MACs → worst case 9,290,304 (exceeds INT16 max 32,767)
   - Conclusion: INT32 accumulator is REQUIRED

8. **Edge Cases**
   - Rounding (ties-to-even): Show 2.5→2, 3.5→4 examples
   - Saturation: Show value exceeding [-128, 127] being clipped
   - Zero-point handling: Emphasize SUBTRACT before MAC, ADD after requantization
   - Padding: Note that zero-padded values should use input zero_point, not 0

9. **Hardware Implementation Notes**
   - Runnable Python pseudocode with explicit type annotations (np.int8, np.int32, np.float32)
   - Comments mapping to hardware stages
   - Per CONTEXT.md: Skip hardware-specific accumulator tracking, focus on quantization math

**Use GitHub MathJax syntax:**
- Display math: $$formula$$
- Inline math: $variable$
- Escape underscores in variable names: y\_scale, x\_zero\_point
  </action>
  <verify>
1. File exists: `test -f docs/quantization/02-qlinearconv.md`
2. Contains two-stage computation: `grep -q "Stage 1" docs/quantization/02-qlinearconv.md && grep -q "Stage 2" docs/quantization/02-qlinearconv.md`
3. Documents all 9 inputs: `grep -c "x_scale\|x_zero_point\|w_scale\|w_zero_point\|y_scale\|y_zero_point" docs/quantization/02-qlinearconv.md` returns >= 6
4. Contains INT32 overflow: `grep -q "INT32\|int32\|overflow" docs/quantization/02-qlinearconv.md`
5. Contains per-channel vs per-tensor table: `grep -q "per-channel\|per_channel" docs/quantization/02-qlinearconv.md && grep -q "per-tensor\|per_tensor" docs/quantization/02-qlinearconv.md`
6. Documents which layers use which: `grep -qi "conv.\+per-\(channel\|tensor\)" docs/quantization/02-qlinearconv.md`
  </verify>
  <done>
QLinearConv documentation exists with:
- Two-stage computation formula with GitHub MathJax
- All 9 inputs documented in table format
- **Per-channel vs per-tensor table showing which ResNet8 layers use which approach**
- Per-tensor worked example with actual ResNet8 values
- Per-channel worked example showing scale array
- INT32 accumulator requirement with overflow demonstration
- Edge cases for rounding, saturation, zero-point handling
  </done>
</task>

<task type="auto">
  <name>Task 3: Create QLinearConv validation script</name>
  <files>scripts/validate_qlinearconv.py</files>
  <action>
Create validation script that confirms manual QLinearConv calculation matches ONNX Runtime output:

**Script structure:**
```python
#!/usr/bin/env python3
"""
Validate QLinearConv manual implementation against ONNX Runtime.

This script:
1. Loads resnet8_int8.onnx model
2. Extracts first QLinearConv node's parameters
3. Computes output manually using two-stage computation
4. Compares against ONNX Runtime output
5. Reports match/mismatch

Usage:
    python scripts/validate_qlinearconv.py models/resnet8_int8.onnx
"""
```

**Implementation requirements:**
- Use onnx.load() to extract model
- Build initializer lookup dict (pattern from extract_operations.py)
- Extract first QLinearConv node's inputs: x, x_scale, x_zero_point, w, w_scale, w_zero_point, y_scale, y_zero_point
- Implement manual_qlinear_conv() function using raw NumPy:
  - Stage 1: INT32 accumulation with zero-point subtraction
  - Stage 2: Scale application, rounding, saturation
- Run ONNX Runtime InferenceSession on same input
- Compare with np.allclose() for floating point tolerance
- For INT8 output: exact match expected (use == comparison)

**Output format:**
```
Loading model: models/resnet8_int8.onnx
Found QLinearConv node: /conv1/Conv_quant

Input parameters:
  x_scale: 0.1234
  x_zero_point: 0
  w_scale: 0.0567
  w_zero_point: 0
  y_scale: 0.0890
  y_zero_point: 0

Manual computation:
  Input shape: (1, 3, 32, 32)
  Output shape: (1, 16, 32, 32)

Validation result: PASS ✓
  Max difference: 0 (exact match)
```

**Error handling:**
- If model doesn't exist: helpful message pointing to Task 1 (generate ONNX model)
- If no QLinearConv nodes: note this (but resnet8_int8.onnx should have them)

**Follow project patterns:**
- argparse for CLI with model_path argument
- Type hints
- Docstrings
- No external dependencies beyond numpy, onnx, onnxruntime
  </action>
  <verify>
1. Script is executable: `python scripts/validate_qlinearconv.py --help` shows usage
2. If model exists, runs without error (may need to check if model file is present first)
3. Contains manual implementation: `grep -q "np.int32\|INT32" scripts/validate_qlinearconv.py`
4. Contains comparison: `grep -q "np.allclose\|exact match" scripts/validate_qlinearconv.py`
  </verify>
  <done>
Validation script exists that:
- Loads ONNX model and extracts QLinearConv parameters
- Implements manual two-stage QLinearConv computation
- Compares manual result against ONNX Runtime
- Reports PASS/FAIL with difference metrics
- Handles missing model gracefully
  </done>
</task>

</tasks>

<verification>
- [ ] models/resnet8_int8.onnx exists (generated or already present)
- [ ] models/resnet8_int8_operations.json exists with QLinearConv entries
- [ ] docs/quantization/02-qlinearconv.md exists with >300 lines (comprehensive)
- [ ] Two-stage computation documented with Stage 1 (MAC) and Stage 2 (requantization)
- [ ] All 9 QLinearConv inputs documented
- [ ] Per-channel vs per-tensor table included showing which ResNet8 layers use which
- [ ] Per-tensor and per-channel examples included with actual values
- [ ] INT32 accumulator overflow demonstrated with runnable code
- [ ] scripts/validate_qlinearconv.py runs successfully
- [ ] GitHub MathJax renders correctly (manual check or CI preview)
</verification>

<success_criteria>
1. ONNX model exists and operations are extracted (honoring user decision for actual values)
2. QLinearConv documentation covers CORE-01 requirement: all 9 inputs, two-stage computation, per-channel handling
3. **Documentation explicitly documents which ResNet8 layers use per-channel vs per-tensor**
4. Documentation includes worked examples with actual ResNet8 values
5. INT32 accumulator requirement is proven, not just stated
6. Validation script demonstrates correctness against ONNX Runtime
7. Documentation follows established pattern from 01-boundary-operations.md
</success_criteria>

<output>
After completion, create `.planning/phases/11-core-operations-documentation/11-01-SUMMARY.md`
</output>
