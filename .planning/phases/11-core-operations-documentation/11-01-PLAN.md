---
phase: 11-core-operations-documentation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docs/quantization/02-qlinearconv.md
  - scripts/validate_qlinearconv.py
autonomous: true

must_haves:
  truths:
    - "QLinearConv formula explicitly shows two-stage computation (INT8×INT8→INT32 MAC, then requantization)"
    - "All 9 inputs documented with types, shapes, and purposes"
    - "Per-tensor and per-channel quantization cases explained with actual ResNet8 values"
    - "INT32 accumulator requirement proven via overflow demonstration code"
    - "Validation script confirms manual calculations match ONNX Runtime output"
  artifacts:
    - path: "docs/quantization/02-qlinearconv.md"
      provides: "Complete QLinearConv reference documentation"
      contains: "two-stage computation"
    - path: "scripts/validate_qlinearconv.py"
      provides: "Verification script for QLinearConv calculations"
      contains: "np.allclose"
  key_links:
    - from: "docs/quantization/02-qlinearconv.md"
      to: "ONNX QLinearConv specification"
      via: "exact formula notation"
      pattern: "y_scale.*y_zero_point"
    - from: "scripts/validate_qlinearconv.py"
      to: "resnet8_int8.onnx"
      via: "onnx.load and ONNX Runtime"
      pattern: "onnx\\.load|InferenceSession"
---

<objective>
Create comprehensive QLinearConv documentation for analog accelerator implementers, covering the two-stage computation pattern with actual ResNet8 values.

Purpose: QLinearConv is the primary operation in quantized ResNet8 (all 8 convolution layers use it). Hardware implementers need to understand exact calculations.
Output: Complete documentation file with formulas, worked examples, and validation code.
</objective>

<execution_context>
@/home/impactaky/shelffiles/config/claude/get-shit-done/workflows/execute-plan.md
@/home/impactaky/shelffiles/config/claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-core-operations-documentation/11-CONTEXT.md
@.planning/phases/11-core-operations-documentation/11-RESEARCH.md
@.planning/phases/10-boundary-operations-documentation/10-01-SUMMARY.md

# Reference existing documentation pattern
@docs/quantization/01-boundary-operations.md

# Phase 9 scripts for extracting ONNX values
@scripts/extract_operations.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create QLinearConv documentation with two-stage computation</name>
  <files>docs/quantization/02-qlinearconv.md</files>
  <action>
Create comprehensive QLinearConv documentation following the established pattern from 01-boundary-operations.md:

**Structure:**
1. **Overview** (3-4 sentences)
   - What QLinearConv does (quantized convolution)
   - Relationship to standard Conv: "same spatial computation, different arithmetic"
   - Key difference: two-stage computation with INT32 accumulator

2. **Input Specification** (table format)
   - All 9 inputs with exact ONNX names: x, x_scale, x_zero_point, w, w_scale, w_zero_point, y_scale, y_zero_point, B (optional bias)
   - Types, shapes, descriptions for each
   - Note: bias (B) is optional (8 required, 1 optional)

3. **Two-Stage Computation Formula**
   - Use GitHub MathJax syntax (escape underscores: y\_scale)
   - Stage 1: INT8×INT8→INT32 MAC operations
     $$\text{acc}_{INT32} = \sum_{k} (x_{INT8}[k] - x\_zero\_point) \times (w_{INT8}[k] - w\_zero\_point)$$
   - Stage 2: Requantization to INT8
     $$y_{INT8} = \text{saturate}\left(\text{round}\left(\text{acc} \times \frac{x\_scale \times w\_scale}{y\_scale}\right) + y\_zero\_point\right)$$
   - Explain WHY this pattern: separates integer ops (hardware-efficient) from scaling (precision-critical)

4. **Per-Tensor Quantization Example**
   - Use first QLinearConv from ResNet8: Conv2D(16, 3×3) on 32×32 input
   - Show actual values from ONNX model (use extract_operations.py or onnx.load)
   - Step through:
     - Input patch extraction (3×3×3 = 27 values)
     - Zero-point subtraction for input and weights
     - INT32 MAC accumulation (show partial sums)
     - Scale factor computation
     - Requantization with saturation
   - Include Python code that readers can run

5. **Per-Channel Quantization Example**
   - Document which ResNet8 layers use per-channel vs per-tensor
   - Use a per-channel layer (likely later conv with more output channels)
   - Show w_scale as array [M] not scalar
   - Demonstrate: Stage 1 (MAC) is identical, only Stage 2 differs per channel
   - Storage overhead calculation: 2*M / (M*C*kH*kW) ≈ negligible

6. **INT32 Accumulator Overflow Demonstration**
   - Include runnable code from RESEARCH.md Pattern 3 (demonstrate_accumulator_overflow)
   - Show worst case: 127 × 127 × (channels × kernel_size) = overflow for INT16
   - For ResNet8: 3×3×64 = 576 MACs → worst case 9,290,304 (exceeds INT16 max 32,767)
   - Conclusion: INT32 accumulator is REQUIRED

7. **Edge Cases**
   - Rounding (ties-to-even): Show 2.5→2, 3.5→4 examples
   - Saturation: Show value exceeding [-128, 127] being clipped
   - Zero-point handling: Emphasize SUBTRACT before MAC, ADD after requantization
   - Padding: Note that zero-padded values should use input zero_point, not 0

8. **Hardware Implementation Notes**
   - Runnable Python pseudocode with explicit type annotations (np.int8, np.int32, np.float32)
   - Comments mapping to hardware stages
   - Per CONTEXT.md: Skip hardware-specific accumulator tracking, focus on quantization math

**Use GitHub MathJax syntax:**
- Display math: $$formula$$
- Inline math: $variable$
- Escape underscores in variable names: y\_scale, x\_zero\_point
  </action>
  <verify>
1. File exists: `test -f docs/quantization/02-qlinearconv.md`
2. Contains two-stage computation: `grep -q "Stage 1" docs/quantization/02-qlinearconv.md && grep -q "Stage 2" docs/quantization/02-qlinearconv.md`
3. Documents all 9 inputs: `grep -c "x_scale\|x_zero_point\|w_scale\|w_zero_point\|y_scale\|y_zero_point" docs/quantization/02-qlinearconv.md` returns >= 6
4. Contains INT32 overflow: `grep -q "INT32\|int32\|overflow" docs/quantization/02-qlinearconv.md`
5. Contains per-channel: `grep -q "per-channel\|per_channel" docs/quantization/02-qlinearconv.md`
  </verify>
  <done>
QLinearConv documentation exists with:
- Two-stage computation formula with GitHub MathJax
- All 9 inputs documented in table format
- Per-tensor worked example with actual ResNet8 values
- Per-channel worked example showing scale array
- INT32 accumulator requirement with overflow demonstration
- Edge cases for rounding, saturation, zero-point handling
  </done>
</task>

<task type="auto">
  <name>Task 2: Create QLinearConv validation script</name>
  <files>scripts/validate_qlinearconv.py</files>
  <action>
Create validation script that confirms manual QLinearConv calculation matches ONNX Runtime output:

**Script structure:**
```python
#!/usr/bin/env python3
"""
Validate QLinearConv manual implementation against ONNX Runtime.

This script:
1. Loads resnet8_int8.onnx model
2. Extracts first QLinearConv node's parameters
3. Computes output manually using two-stage computation
4. Compares against ONNX Runtime output
5. Reports match/mismatch

Usage:
    python scripts/validate_qlinearconv.py models/resnet8_int8.onnx
"""
```

**Implementation requirements:**
- Use onnx.load() to extract model
- Build initializer lookup dict (pattern from extract_operations.py)
- Extract first QLinearConv node's inputs: x, x_scale, x_zero_point, w, w_scale, w_zero_point, y_scale, y_zero_point
- Implement manual_qlinear_conv() function using raw NumPy:
  - Stage 1: INT32 accumulation with zero-point subtraction
  - Stage 2: Scale application, rounding, saturation
- Run ONNX Runtime InferenceSession on same input
- Compare with np.allclose() for floating point tolerance
- For INT8 output: exact match expected (use == comparison)

**Output format:**
```
Loading model: models/resnet8_int8.onnx
Found QLinearConv node: /conv1/Conv_quant

Input parameters:
  x_scale: 0.1234
  x_zero_point: 0
  w_scale: 0.0567
  w_zero_point: 0
  y_scale: 0.0890
  y_zero_point: 0

Manual computation:
  Input shape: (1, 3, 32, 32)
  Output shape: (1, 16, 32, 32)

Validation result: PASS ✓
  Max difference: 0 (exact match)
```

**Error handling:**
- If model doesn't exist: helpful message pointing to Phase 6 output
- If no QLinearConv nodes: note this (but resnet8_int8.onnx should have them)

**Follow project patterns:**
- argparse for CLI with model_path argument
- Type hints
- Docstrings
- No external dependencies beyond numpy, onnx, onnxruntime
  </action>
  <verify>
1. Script is executable: `python scripts/validate_qlinearconv.py --help` shows usage
2. If model exists, runs without error (may need to check if model file is present first)
3. Contains manual implementation: `grep -q "np.int32\|INT32" scripts/validate_qlinearconv.py`
4. Contains comparison: `grep -q "np.allclose\|exact match" scripts/validate_qlinearconv.py`
  </verify>
  <done>
Validation script exists that:
- Loads ONNX model and extracts QLinearConv parameters
- Implements manual two-stage QLinearConv computation
- Compares manual result against ONNX Runtime
- Reports PASS/FAIL with difference metrics
- Handles missing model gracefully
  </done>
</task>

</tasks>

<verification>
- [ ] docs/quantization/02-qlinearconv.md exists with >300 lines (comprehensive)
- [ ] Two-stage computation documented with Stage 1 (MAC) and Stage 2 (requantization)
- [ ] All 9 QLinearConv inputs documented
- [ ] Per-tensor and per-channel examples included
- [ ] INT32 accumulator overflow demonstrated with runnable code
- [ ] scripts/validate_qlinearconv.py runs successfully (if ONNX model available)
- [ ] GitHub MathJax renders correctly (manual check or CI preview)
</verification>

<success_criteria>
1. QLinearConv documentation covers CORE-01 requirement: all 9 inputs, two-stage computation, per-channel handling
2. Documentation includes worked examples with actual ResNet8 values (or realistic values if model unavailable)
3. INT32 accumulator requirement is proven, not just stated
4. Validation script demonstrates correctness against ONNX Runtime
5. Documentation follows established pattern from 01-boundary-operations.md
</success_criteria>

<output>
After completion, create `.planning/phases/11-core-operations-documentation/11-01-SUMMARY.md`
</output>
