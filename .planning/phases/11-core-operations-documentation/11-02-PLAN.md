---
phase: 11-core-operations-documentation
plan: 02
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - docs/quantization/03-qlinearmatmul.md
  - scripts/validate_qlinearmatmul.py
autonomous: true

must_haves:
  truths:
    - "QLinearMatMul formula shows two-stage computation matching QLinearConv pattern"
    - "All 8 inputs documented with types and purposes"
    - "Worked example uses ResNet8 final FC layer values"
    - "Relationship to QLinearConv explained (same pattern, no spatial dimensions)"
    - "Explicit cross-reference link to QLinearConv documentation included"
    - "Validation script confirms manual calculations match ONNX Runtime"
  artifacts:
    - path: "docs/quantization/03-qlinearmatmul.md"
      provides: "Complete QLinearMatMul reference documentation"
      contains: "two-stage computation"
    - path: "scripts/validate_qlinearmatmul.py"
      provides: "Verification script for QLinearMatMul calculations"
      contains: "np.allclose"
  key_links:
    - from: "docs/quantization/03-qlinearmatmul.md"
      to: "docs/quantization/02-qlinearconv.md"
      via: "explicit markdown cross-reference link"
      pattern: "\\[.*QLinearConv.*\\]\\(02-qlinearconv\\.md"
    - from: "scripts/validate_qlinearmatmul.py"
      to: "resnet8_int8.onnx"
      via: "onnx.load and ONNX Runtime"
      pattern: "onnx\\.load|InferenceSession"
---

<objective>
Create QLinearMatMul documentation for the ResNet8 FC layer, building on the two-stage computation pattern established in QLinearConv.

Purpose: QLinearMatMul handles the final fully-connected layer in quantized ResNet8 (64→10). Simpler than QLinearConv (no spatial dimensions) but same fundamental pattern.
Output: Complete documentation file with formulas, worked example, and validation code.
</objective>

<execution_context>
@/home/impactaky/shelffiles/config/claude/get-shit-done/workflows/execute-plan.md
@/home/impactaky/shelffiles/config/claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-core-operations-documentation/11-CONTEXT.md
@.planning/phases/11-core-operations-documentation/11-RESEARCH.md

# Prior plan from this phase (for patterns, cross-references)
@.planning/phases/11-core-operations-documentation/11-01-SUMMARY.md

# Reference existing documentation patterns
@docs/quantization/01-boundary-operations.md
@docs/quantization/02-qlinearconv.md

# ONNX model and extracted operations (from Plan 01)
@models/resnet8_int8.onnx
@models/resnet8_int8_operations.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create QLinearMatMul documentation with explicit cross-references</name>
  <files>docs/quantization/03-qlinearmatmul.md</files>
  <action>
Create QLinearMatMul documentation following established patterns, keeping it concise since it shares the two-stage pattern with QLinearConv:

**Structure:**

1. **Overview** (3-4 sentences)
   - What QLinearMatMul does (quantized matrix multiplication)
   - Relationship to QLinearConv: same two-stage pattern, no spatial dimensions
   - Use case in CNNs: fully-connected layers (ResNet8 has one: 64→10)

2. **Input Specification** (table format)
   - All 8 inputs with exact ONNX names: a, a_scale, a_zero_point, b, b_scale, b_zero_point, y_scale, y_zero_point
   - Note: No bias input (unlike QLinearConv)
   - Note naming difference: a/b instead of x/w (ONNX convention)

3. **Two-Stage Computation Formula**
   - **MANDATORY CROSS-REFERENCE:** Include explicit markdown link:
     > For detailed explanation of the two-stage computation pattern, see [QLinearConv documentation](02-qlinearconv.md#two-stage-computation).
   - Show formula for completeness:
   - Stage 1: INT8×INT8→INT32 MAC
     $$\text{acc}_{INT32}[i,j] = \sum_{k} (a_{INT8}[i,k] - a\_zero\_point) \times (b_{INT8}[k,j] - b\_zero\_point)$$
   - Stage 2: Requantization to INT8
     $$y_{INT8}[i,j] = \text{saturate}\left(\text{round}\left(\text{acc}[i,j] \times \frac{a\_scale \times b\_scale}{y\_scale}\right) + y\_zero\_point\right)$$
   - Note: Identical pattern to QLinearConv, just matrix indices instead of spatial

4. **Comparison to QLinearConv**
   - **MANDATORY CROSS-REFERENCE:** Include explicit markdown link in the section:
     > The [QLinearConv operation](02-qlinearconv.md) shares the same fundamental computation pattern.
   - Table showing what's same vs different:
     | Aspect | QLinearConv | QLinearMatMul |
     |--------|-------------|---------------|
     | Stage 1 pattern | INT8×INT8→INT32 MAC | Same |
     | Stage 2 pattern | Scale + requantize | Same |
     | Inputs | 9 (with optional bias) | 8 (no bias) |
     | Spatial dimensions | Yes (H, W) | No |
     | Naming | x, w | a, b |
     | Per-channel | Common | Less common |

5. **ResNet8 FC Layer Example**
   - ResNet8 final layer: Input 64 features → Output 10 classes
   - After average pooling: shape (batch, 64)
   - Weight matrix: shape (64, 10)
   - Show actual values from ONNX model (use `models/resnet8_int8_operations.json` from Plan 01)
   - Step through for one output element:
     - 64 INT8×INT8 multiplications
     - INT32 accumulation (max value: 127 × 127 × 64 = 1,032,256, well within INT32)
     - Scale factor application
     - Requantization

6. **INT32 Accumulator for MatMul**
   - ResNet8 FC: 64 MACs per output
   - Worst case: 127 × 127 × 64 = 1,032,256
   - INT16 max: 32,767 — would overflow!
   - Same conclusion as QLinearConv: INT32 required
   - **CROSS-REFERENCE:** Link to overflow demonstration in QLinearConv:
     > See [INT32 accumulator demonstration](02-qlinearconv.md#int32-accumulator-overflow-demonstration) for detailed proof.

7. **Hardware Implementation Notes**
   - Per CONTEXT.md: Skip detailed hardware tracking, focus on quantization math
   - Simpler than QLinearConv: no stride, padding, kernel extraction
   - Standard matrix multiplication loop with INT32 accumulator

**Keep it concise:**
- This is simpler than QLinearConv
- Reference QLinearConv for shared patterns instead of duplicating
- Focus on what's different (input names, no spatial dims, no bias)

**CRITICAL: Cross-reference verification**
The documentation MUST include at least TWO explicit markdown links to 02-qlinearconv.md:
1. In the Two-Stage Computation section
2. In the Comparison section or INT32 section
  </action>
  <verify>
1. File exists: `test -f docs/quantization/03-qlinearmatmul.md`
2. Contains two-stage computation: `grep -q "Stage 1" docs/quantization/03-qlinearmatmul.md && grep -q "Stage 2" docs/quantization/03-qlinearmatmul.md`
3. Documents all 8 inputs: `grep -c "a_scale\|a_zero_point\|b_scale\|b_zero_point\|y_scale\|y_zero_point" docs/quantization/03-qlinearmatmul.md` returns >= 6
4. **Contains explicit cross-reference link:** `grep -qE "\[.*QLinearConv.*\]\(02-qlinearconv\.md" docs/quantization/03-qlinearmatmul.md`
5. Contains INT32: `grep -q "INT32\|int32" docs/quantization/03-qlinearmatmul.md`
  </verify>
  <done>
QLinearMatMul documentation exists with:
- Two-stage computation formula with GitHub MathJax
- All 8 inputs documented in table format
- Comparison table showing relationship to QLinearConv
- **Explicit markdown cross-reference links to 02-qlinearconv.md**
- Worked example using ResNet8 FC layer (64→10) with actual ONNX values
- INT32 accumulator requirement confirmed
  </done>
</task>

<task type="auto">
  <name>Task 2: Create QLinearMatMul validation script</name>
  <files>scripts/validate_qlinearmatmul.py</files>
  <action>
Create validation script following the pattern from validate_qlinearconv.py:

**Script structure:**
```python
#!/usr/bin/env python3
"""
Validate QLinearMatMul manual implementation against ONNX Runtime.

This script:
1. Loads resnet8_int8.onnx model
2. Extracts QLinearMatMul node's parameters (FC layer)
3. Computes output manually using two-stage computation
4. Compares against ONNX Runtime output
5. Reports match/mismatch

Usage:
    python scripts/validate_qlinearmatmul.py models/resnet8_int8.onnx
"""
```

**Implementation requirements:**
- Use onnx.load() to extract model
- Find QLinearMatMul node (should be one for FC layer)
- Extract all 8 inputs: a, a_scale, a_zero_point, b, b_scale, b_zero_point, y_scale, y_zero_point
- Implement manual_qlinear_matmul() function using raw NumPy:
  - Stage 1: INT32 accumulation with zero-point subtraction
  - Stage 2: Scale application, rounding, saturation
- Run ONNX Runtime on same input
- Compare results

**Key difference from QLinearConv:**
- Simpler: just matrix multiplication, no convolution loops
- Input names: a, b instead of x, w
- No bias handling

**Output format:**
```
Loading model: models/resnet8_int8.onnx
Found QLinearMatMul node: /fc/MatMul_quant

Input parameters:
  a_scale: 0.1234
  a_zero_point: 0
  b_scale: 0.0567
  b_zero_point: 0
  y_scale: 0.0890
  y_zero_point: 0

Manual computation:
  Input shape: (1, 64)
  Weight shape: (64, 10)
  Output shape: (1, 10)

Validation result: PASS ✓
  Max difference: 0 (exact match)
```

**Follow project patterns from validate_qlinearconv.py:**
- Same argparse structure
- Same output format
- Same error handling
  </action>
  <verify>
1. Script is executable: `python scripts/validate_qlinearmatmul.py --help` shows usage
2. Contains manual implementation: `grep -q "np.int32\|INT32" scripts/validate_qlinearmatmul.py`
3. Contains comparison: `grep -q "np.allclose\|exact match" scripts/validate_qlinearmatmul.py`
4. Uses correct input names: `grep -q "a_scale\|b_scale" scripts/validate_qlinearmatmul.py`
  </verify>
  <done>
Validation script exists that:
- Loads ONNX model and extracts QLinearMatMul parameters
- Implements manual two-stage QLinearMatMul computation
- Compares manual result against ONNX Runtime
- Reports PASS/FAIL with difference metrics
- Follows patterns from validate_qlinearconv.py
  </done>
</task>

</tasks>

<verification>
- [ ] docs/quantization/03-qlinearmatmul.md exists
- [ ] Two-stage computation documented with proper formulas
- [ ] All 8 QLinearMatMul inputs documented
- [ ] **Explicit cross-reference links to 02-qlinearconv.md present** (grep confirms pattern `[.*QLinearConv.*](02-qlinearconv.md`)
- [ ] ResNet8 FC layer example included (64→10) with actual ONNX values
- [ ] scripts/validate_qlinearmatmul.py runs successfully
- [ ] Documentation is concise, not duplicating QLinearConv content
</verification>

<success_criteria>
1. QLinearMatMul documentation covers CORE-02 requirement: inputs, computation stages, hardware requirements
2. **Documentation includes explicit markdown cross-reference links to QLinearConv**
3. Worked example uses actual ResNet8 FC layer values from ONNX model (64→10)
4. Validation script demonstrates correctness against ONNX Runtime
5. Combined with Plan 01, covers CORE-03: worked examples for every layer type in ResNet8
</success_criteria>

<output>
After completion, create `.planning/phases/11-core-operations-documentation/11-02-SUMMARY.md`
</output>
